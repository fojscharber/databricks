{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16117843-61e7-4e8e-b0c9-dcb2d041c68f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**This Script will create a dynamic view - FisheryInsights based on the dynamic nature**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66a7c78a-a9c9-41a3-9b18-fc5cdd44ea29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>databaseName</th></tr></thead><tbody><tr><td>_fivetran_setup_test</td></tr><tr><td>_fivetran_staging</td></tr><tr><td>aquamanager_growout_dbo</td></tr><tr><td>aquamanager_hatchery_dbo</td></tr><tr><td>default</td></tr><tr><td>fivetran_log</td></tr><tr><td>google_sheets</td></tr><tr><td>limble</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "_fivetran_setup_test"
        ],
        [
         "_fivetran_staging"
        ],
        [
         "aquamanager_growout_dbo"
        ],
        [
         "aquamanager_hatchery_dbo"
        ],
        [
         "default"
        ],
        [
         "fivetran_log"
        ],
        [
         "google_sheets"
        ],
        [
         "limble"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 1
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "databaseName",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "show databases;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d340dd-4ff9-4fd3-a138-058848a546b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "broodstockorigin = spark.sql(f\"\"\" SELECT DISTINCT UPPER(broodstockorigin) as broodstockorigin FROM aquamanager_hatchery_dbo.tltransdetails\n",
    "where broodstockorigin IS NOT NULL AND TRIM(broodstockorigin) != ''\n",
    "\"\"\").select('broodstockorigin').rdd.flatMap(lambda x: x).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ed8d4a-15a6-4bd6-b789-157abb46ac26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\n",
    "    name=\"broodstockorigin\",\n",
    "    label=\"broodstockorigin\",\n",
    "    choices= broodstockorigin,\n",
    "    defaultValue=\"ECUADOR\"\n",
    ")\n",
    "\n",
    "dbutils.widgets.multiselect(\n",
    "    name=\"Reception\",\n",
    "    label=\"Reception\",\n",
    "    choices=[\"7B\", \"8A\", \"Batch 6\",\"Batch 6B\", \"Batch 7\", \"Batch 7A\", \"Batch 7B\", \"Cohort 8A\", \"Cohorte 8A\"],\n",
    "    defaultValue=\"Batch 7A\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42d0ca16-d685-4de3-bbc3-f995ccb393d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dynamic Variable assignment \n",
    "broodstockorigin = dbutils.widgets.get(\"broodstockorigin\").strip()\n",
    "Reception = dbutils.widgets.get(\"Reception\").strip()\n",
    "# Reception\n",
    "\n",
    "\n",
    "elements = Reception.split(',')\n",
    "\n",
    "# Surround each element with double quotes and join them back together\n",
    "Reception = ','.join([\"'{}'\".format(element) for element in elements])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492bfa1f-4862-4246-8304-1369d7ee9d5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The View creation is running for broodstockorigin = ECUADOR and Reception = 'Batch 7A','Batch 7B','Cohort 8A','Batch 6B','7B' \n"
     ]
    }
   ],
   "source": [
    "#Loggers:\n",
    "print(f\"The View creation is running for broodstockorigin = {broodstockorigin} and Reception = {Reception} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70619d64-c7ee-4594-8cc2-3edc1e5eccbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The `extractAndConcatUDF` is a User-Defined Function (UDF) written in Scala. It's designed to process input strings, typically containing specific patterns. In this case, it identifies uppercase alphanumeric sequences followed  by \"Ecuador /\" and joins them together using hyphens ('-'). This UDF is valuable for parsing and extracting structured information from text data, enhancing data processing capabilities in SQL queries.\n",
    "\n",
    "**Usage:** This Function is used to extract broodstock_origen from broodstocktanks.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "Example 1:  \n",
    "**Input:** M5 Ecuador / RH6 CENAIM  \n",
    "**Output:** RH6\n",
    "\n",
    "Example 2:  \n",
    "**Input:** M5 Ecuador / RH6 CENAIM|M6 Ecuador / RH7 CENAIM|RAS 6 Ecuador / RF3|M3 Ecuador / RF1  \n",
    "**Output:** RF1-RF3-RH6-RH7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fa8d668-1e3e-4096-9ab1-1a07a754ad60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">import org.apache.spark.sql.functions.udf\n",
       "extractAndConcatUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$11439/1285416725@52f58565,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),None,true,true)\n",
       "res0: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$11439/1285416725@52f58565,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),Some(extractAndConcat),true,true)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">import org.apache.spark.sql.functions.udf\nextractAndConcatUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$11439/1285416725@52f58565,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),None,true,true)\nres0: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$11439/1285416725@52f58565,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),Some(extractAndConcat),true,true)\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "// Define a User-Defined Function (UDF) named \"extractAndConcat\"\n",
    "val extractAndConcatUDF = udf((input: String) => {\n",
    "  if (input != null) {\n",
    "    // Define a regular expression pattern to match data (case-insensitive)\n",
    "    val pattern = \"\"\"(?i)[A-Z0-9]+ Ecuador / ([A-Z0-9]+)\"\"\".r\n",
    "\n",
    "    // Find the first match of the pattern in the input string\n",
    "    val matched = pattern.findFirstMatchIn(input)\n",
    "\n",
    "    matched match {\n",
    "      case Some(m) => m.group(1).toUpperCase  // Return the matched group in uppercase\n",
    "      case None if input.matches(\"[mM]6\") => \"M6\"  // Return \"M6\" for \"m6\" or \"M6\"\n",
    "      case _ => null.asInstanceOf[String]  // Return null for other cases\n",
    "    }\n",
    "  } else {\n",
    "    // Return null if the input is null\n",
    "    null.asInstanceOf[String]\n",
    "  }\n",
    "})\n",
    "\n",
    "// Register the UDF\n",
    "spark.udf.register(\"extractAndConcat\", extractAndConcatUDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19aee78e-fe68-4355-a34e-db240102193e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "The `removeDuplicatesUDF` is a User-Defined Function (UDF) written in Scala for Apache Spark. It's designed to process input strings by splitting them using hyphens ('-') to separate values. The UDF then finds and retains distinct (unique) values from the split values and rejoins them together using hyphens. This UDF is particularly useful for deduplicating values in a string. It enhances data processing capabilities within Spark SQL queries.\n",
    "\n",
    "**Usage:** This Function is used in creation of broodstock_origen column.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "Example 1:  \n",
    "**Input:** offshore-offshore  \n",
    "**Output:** offshore\n",
    "\n",
    "Example 2:  \n",
    "**Input:** offshore-nursery-nursery-larvae  \n",
    "**Output:** offshore-nursery-larvae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8218be2-82d2-4d3e-8578-5f813dc68b8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">removeDuplicatesUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$11440/997353892@10317ed4,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),None,true,true)\n",
       "res1: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$11440/997353892@10317ed4,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),Some(removeDuplicates),true,true)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">removeDuplicatesUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$11440/997353892@10317ed4,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),None,true,true)\nres1: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$11440/997353892@10317ed4,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),Some(removeDuplicates),true,true)\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// Register the UDF to remove duplicate values\n",
    "\n",
    "// Define a User-Defined Function (UDF) named \"removeDuplicates\"\n",
    "val removeDuplicatesUDF = udf((origen: String) => {\n",
    "  // Split the input string by hyphen \"-\" to separate values\n",
    "  val splitValues = origen.split(\"-\")\n",
    "  \n",
    "  // Find distinct (unique) values from the splitValues array\n",
    "  val uniqueValues = splitValues.distinct.sorted\n",
    "  \n",
    "  // Join the unique values back together with hyphens \"-\"\n",
    "  uniqueValues.mkString(\"-\")\n",
    "})\n",
    "\n",
    "// Register the UDF with a name \"removeDuplicates\" for use in Spark SQL queries\n",
    "spark.udf.register(\"removeDuplicates\", removeDuplicatesUDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d997d7e-b1a3-487e-9ec0-f179c3b741ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The `countIndividualValuesUDF` is a User-Defined Function (UDF) written in Scala for Apache Spark. It's designed to process input strings by splitting them using hyphens ('-') to separate values. The UDF then counts the occurrences of each value and presents the counts as a comma-separated string. This UDF enhances data processing capabilities within Spark SQL queries for calculating the occurrence counts of individual values.\n",
    "\n",
    "**Usage:** This Function is used in creation of Tank column.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "Example 1:  \n",
    "**Input:** RF1-RF1-RH3  \n",
    "**Output:** 2,1\n",
    "\n",
    "Example 2:  \n",
    "**Input:** RF1-RF1-RH2-RH2-RF1  \n",
    "**Output:** 3,2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aa86c55-06ef-4b18-8b50-54ef7b6a0caa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">countIndividualValuesUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$11441/1063736899@1d526ef4,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),None,true,true)\n",
       "res2: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$11441/1063736899@1d526ef4,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),Some(countIndividualValues),true,true)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">countIndividualValuesUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$11441/1063736899@1d526ef4,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),None,true,true)\nres2: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$11441/1063736899@1d526ef4,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),Some(countIndividualValues),true,true)\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "\n",
    "// Define the UDF\n",
    "val countIndividualValuesUDF = udf((origen: String) => {\n",
    "  // Split the input string by \"-\"\n",
    "  val splitValues = origen.split(\"-\")\n",
    "  \n",
    "  // Initialize a map to store counts\n",
    "  var countsMap = scala.collection.mutable.Map[String, Int]().withDefaultValue(0)\n",
    "\n",
    "  // Iterate through the split values and count occurrences\n",
    "  for (value <- splitValues) {\n",
    "    countsMap(value) += 1\n",
    "  }\n",
    "\n",
    "  // Convert the counts map to a comma-separated string\n",
    "  val countsString = countsMap.map { case (key, value) => s\"$value\" }.mkString(\",\")\n",
    "\n",
    "  countsString\n",
    "})\n",
    "\n",
    "// Register the UDF with a name\n",
    "spark.udf.register(\"countIndividualValues\", countIndividualValuesUDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "227c5e7f-7b40-4e74-a318-117796a43422",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# This cell demonstrates the creation of a view named FisheryInsights using Spark SQL.\n",
    "# The following SQL code defines a Common Table Expression (CTE) named BASE that joins multiple tables \n",
    "# and calculates various statistics related to fishery data. It creates calculated columns, applies conditions,\n",
    "# and generates observations based on different status values. Finally, it groups and orders the results. \n",
    "# The view FisheryInsights is created based on the result of the SQL query and can be used for further analysis.\n",
    "FisheryInsightsDf = spark.sql(f\"\"\"\n",
    "-- Common Table Expression (CTE) named BASE\n",
    "WITH BASE AS (\n",
    "-- Select relevant columns and apply transformations\n",
    "select\n",
    "    l.designation as Reception,\n",
    "    tl.spawningdate as Date,\n",
    "    -- l.stockdate as Date,\n",
    "    extractAndConcat(ttd.broodstocktanks) as broodstock_origen,\n",
    "    ttd.eggno,\n",
    "    ttd.initialeggno,\n",
    "    ttd.broodstocktanks,\n",
    "    ttd.broodstockorigin,\n",
    "    tl.survivalrateafterhatching,\n",
    "    tk.designation as activity,\n",
    "    tk.designation as Broodstock_origen1,\n",
    "    tk.tankvolume * 5800 as tankvolume_in_litres,\n",
    "    CASE\n",
    "            WHEN lps.designation IN ('Nursery', 'Incubacion') THEN 'nursery'\n",
    "            WHEN lps.designation IN ('Coalimentacion rotifero / artemia', 'Alimentacion pellet', 'Alimentacion artemia', 'Eclosion', 'Primera alimentacion Rotiferos', 'Destete') THEN 'larvae'\n",
    "            ELSE 'offshore'\n",
    "        END AS status,\n",
    "    th.age as DPH,\n",
    "    COALESCE(th.netqty,0 )as Larvae_harvest,\n",
    "    th.comments\n",
    "\n",
    "from aquamanager_hatchery_dbo.tanklot tl\n",
    "left join aquamanager_hatchery_dbo.lot l\n",
    "on tl.lotid = l.lotid\n",
    "\n",
    "left join aquamanager_hatchery_dbo.tank tk\n",
    "on tl.tankid = tk.tankid\n",
    "\n",
    "left join aquamanager_hatchery_dbo.tltransdetails ttd\n",
    "on tl.tanklotid = ttd.tanklotid\n",
    "\n",
    "left join aquamanager_hatchery_dbo.broodstockcostmain bscm\n",
    "on tk.tankid = bscm.tankid\n",
    "\n",
    "left join aquamanager_hatchery_dbo.tlharvest th\n",
    "on tl.tanklotid = th.tanklotid\n",
    "\n",
    "left join aquamanager_hatchery_dbo.larvaeproductionstage lps   \n",
    "on tl.larvaeproductionstageid = lps.larvaeproductionstageid\n",
    "\n",
    "where UPPER(ttd.broodstockorigin) = '{broodstockorigin}'\n",
    "and  l.designation in ({Reception})\n",
    "-- and tk.designation like 'Incubador%'\n",
    ")\n",
    "SELECT \n",
    "    -- Batch Number\n",
    "    Reception,\n",
    "    -- Spawning Date\n",
    "    Date,\n",
    "    removeDuplicates(concat_ws('-', collect_list(broodstock_origen))) AS broodstock_origen, \n",
    "    cast(SUM(initialeggno) as INT) AS Eggs_from,\n",
    "    SUM(CASE WHEN activity LIKE 'Incubador%' THEN eggno ELSE 0 END) AS Incubation_stocking_egg_no,\n",
    "    concat(bround((SUM(CASE WHEN activity LIKE 'Incubador%' THEN eggno ELSE 0 END) / SUM(initialeggno)  * 100 ),2) ,'%' ) AS percentage_Hatching_eggs,\n",
    "    (SUM(CASE WHEN activity LIKE 'Incubador%' THEN eggno ELSE 0 END) - SUM(CASE WHEN NOT activity LIKE 'Incubador%' THEN eggno ELSE 0 END))  as Incubation_Mortality,\n",
    "    SUM(CASE WHEN NOT activity LIKE 'Incubador%' THEN eggno ELSE 0 END) AS Survival_Incubation_larvae,\n",
    "    concat(bround(((SUM(CASE WHEN activity LIKE 'Incubador%' THEN eggno ELSE 0 END) - SUM(CASE WHEN NOT activity LIKE 'Incubador%' THEN eggno ELSE 0 END)) / SUM(CASE WHEN activity LIKE 'Incubador%' THEN eggno ELSE 0 END) * 100) ,2) ,'%') as percentageAccumulatedMortality,\n",
    "    concat(bround((SUM(CASE WHEN NOT activity LIKE 'Incubador%' THEN eggno ELSE 0 END) / SUM(CASE WHEN activity LIKE 'Incubador%' THEN eggno ELSE 0 END) * 100 ),2),'%' ) as percentage_Survival_egg_DPH2,\n",
    "    SUM(CASE WHEN NOT activity LIKE 'Incubador%' THEN eggno ELSE 0 END) AS Stoking_tank_larvae,\n",
    "    INT(SUM(CASE WHEN NOT activity LIKE 'Incubador%' THEN eggno ELSE 0 END)/sum(tankvolume_in_litres)) as `Larvae/lt`,\n",
    "    SUM(Larvae_harvest) AS Larvae_harvest,\n",
    "    countIndividualValues (concat_ws('-', collect_list(broodstock_origen))) AS Tank,\n",
    "    \n",
    "    --Survival_rate% = (Incubation_stocking_eggs - incubation_larvae - larvae_harvest) / eggs_from * 100\n",
    "    -- concat(bround((SUM(CASE WHEN activity LIKE 'Incubador%' THEN eggno ELSE 0 END) \n",
    "    -- - SUM(CASE WHEN NOT activity LIKE 'Incubador%' THEN eggno ELSE 0 END)\n",
    "    -- - SUM(Larvae_harvest) ) \n",
    "    -- / cast(SUM(initialeggno) as INT) * 100 ,2) , '%') as Survival_rate_egg_1,\n",
    "\n",
    "   --Survival_rate% = Larvae_harvest  / Incubation_stocking_egg_no * 100   \n",
    "    concat(bround(((SUM(Larvae_harvest))/(SUM(CASE WHEN activity LIKE 'Incubador%' THEN eggno ELSE 0 END))) * 100 ,2), '%') AS Survival_rate_egg,\n",
    "    concat(COALESCE((SUM(Larvae_harvest)/SUM(CASE WHEN NOT activity LIKE 'Incubador%' THEN eggno ELSE 0 END)) * 100 ,0),'%')\n",
    "    AS Survival_rate_larvae,\n",
    "    max(status) as Status,\n",
    "    --removeDuplicates(concat_ws('-', collect_list(status))) AS Status, \n",
    "    COALESCE(AVG(DPH),0) AS DPH , --days prior hatching\n",
    "    CASE\n",
    "    WHEN max(status) = 'nursery' THEN 'The fish are in the nursery area'\n",
    "    WHEN max(status) = 'larvae' THEN 'The fish are in the larvae feeding'\n",
    "    WHEN max(status) = 'offshore-nursery' THEN 'The fish are in the offshore-nursery area'\n",
    "    WHEN max(status) = 'nursery-offshore' THEN 'The fish are in the nursery-offshore area'\n",
    "    ELSE 'The fish are in the offshore' END AS Observations\n",
    "    FROM BASE\n",
    "    GROUP BY Reception,Date\n",
    "    ORDER BY Date,broodstock_origen\n",
    "\"\"\")\n",
    "#.createOrReplaceTempView(\"FisheryInsights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04206987-5850-4475-bb9d-62d6d7aba360",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-273668221091185>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mFisheryInsightsDf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdelta\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartitionBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mReception\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mappend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maquamanager_hatchery_dbo.view_hatchery_larvae\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1586\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1584\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   1585\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n",
       "\u001B[0;32m-> 1586\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: The provided partitioning does not match of the table.\n",
       " - provided: identity(Reception)\n",
       " - table: "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\nFile \u001B[0;32m<command-273668221091185>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mFisheryInsightsDf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdelta\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartitionBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mReception\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mappend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maquamanager_hatchery_dbo.view_hatchery_larvae\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1586\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1584\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1585\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[0;32m-> 1586\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: The provided partitioning does not match of the table.\n - provided: identity(Reception)\n - table: ",
       "errorSummary": "<span class='ansi-red-fg'>IllegalArgumentException</span>: requirement failed: The provided partitioning does not match of the table.\n - provided: identity(Reception)\n - table: ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "FisheryInsightsDf.write.format(\"delta\").partitionBy(\"Reception\").mode(\"append\").saveAsTable(\"aquamanager_hatchery_dbo.view_hatchery_larvae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79c9d72-d57a-4e56-940f-67ecea566623",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Below, we have created a cell to showcase the view **FisheryInsights** created in the cell above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee45bc95-0e3c-4f94-8182-689c1c54a686",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#spark.sql(\"CREATE OR REPLACE TABLE aquamanager_hatchery_dbo.FisheryInsights AS SELECT * FROM FisheryInsights\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2202567380881774,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "view_hatchery_larvae",
   "widgets": {
    "Reception": {
     "currentValue": "Batch 7A,Batch 7B,Cohort 8A,Batch 6B,7B",
     "nuid": "3e83efd3-6934-4dad-8623-31cd02196c26",
     "widgetInfo": {
      "widgetType": "multiselect",
      "defaultValue": "Batch 7A",
      "label": "Reception",
      "name": "Reception",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "7B",
        "8A",
        "Batch 6",
        "Batch 6B",
        "Batch 7",
        "Batch 7A",
        "Batch 7B",
        "Cohort 8A",
        "Cohorte 8A"
       ]
      }
     }
    },
    "broodstockorigin": {
     "currentValue": "ECUADOR",
     "nuid": "d8292774-b320-47e8-8a01-058ea10dd83d",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "ECUADOR",
      "label": "broodstockorigin",
      "name": "broodstockorigin",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "MEXICO/ECUADOR",
        "MEXICO",
        "MEXICO/ECUADOR|MEXICO",
        "MEXICO|ECUADOR",
        "ECUADOR",
        "PANAMA"
       ]
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

