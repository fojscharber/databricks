{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f308c6c-5712-44cd-8385-113751979e40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "watermark = spark.conf.get('spark.custom.gold.watermark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "201d17f5-b261-4bd4-9240-a2370360451b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name='water_quality_aggregations',\n",
    "    comment=\"\"\"\n",
    "        Aggregated 5 minute Water Quality Readings from C2 Avalon Panama Operations\n",
    "\n",
    "        Water quality data points for water quality sensors: Exo Sonde YSI, Manta Sonde, and the \n",
    "        TChain sensor aggregated every 5 minutes. This dataset is scheduled to update every day\n",
    "    \"\"\",\n",
    "    path='/delta/water_quality_aggregations',\n",
    "    partition_cols=[\"site_id\", \"year\", \"month\", \"day\"],\n",
    "    spark_conf={\n",
    "        'spark.sql.sources.partitionOverwriteMode': 'dynamic',\n",
    "        'spark.sql.session.timeZone': 'UTC'\n",
    "    },\n",
    "    table_properties={\n",
    "        'pipelines.autoOptimize.managed': 'true',\n",
    "        'pipelines.autoOptimize.zOrderCols': 'sensor,timestamp',\n",
    "        'pipelines.reset.allowed': 'false'\n",
    "    }\n",
    ")\n",
    "def gold_table():\n",
    "    df = spark\\\n",
    "        .read\\\n",
    "        .format('delta')\\\n",
    "        .table('LIVE.water_quality_1095_days')\n",
    "\n",
    "    df = df.withColumn('date', f.to_date(f.col('timestamp')))\n",
    "    df = df.filter(f'date >= date_sub(current_date(), {watermark})')\n",
    "\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df.groupBy(\n",
    "        f.col(\"farm_code\"),\n",
    "        f.col(\"site_id\"),\n",
    "        f.col(\"sensor\"),\n",
    "        f.col(\"reading\"),\n",
    "        f.window('timestamp', '5 minutes'),\n",
    "    )\\\n",
    "    .agg(\n",
    "        f.mean('value').alias('avg'),\n",
    "        f.percentile_approx('value', 0.5).alias('median'),\n",
    "        f.sum('value').alias('sum'),\n",
    "        f.min('value').alias('min'),\n",
    "        f.max('value').alias('max'),\n",
    "        f.stddev('value').alias('std'),\n",
    "        f.count('value').alias('total')\n",
    "    )\\\n",
    "    .withColumn('metrics', f.struct(\n",
    "        f.col('avg'),\n",
    "        f.col('sum'),\n",
    "        f.col('min'),\n",
    "        f.col('max'),\n",
    "        f.col('std'),\n",
    "        f.col('total'),\n",
    "    ))\\\n",
    "    .withColumn('year', f.year('window.start'))\\\n",
    "    .withColumn('month', f.month('window.start'))\\\n",
    "    .withColumn('day', f.dayofmonth('window.start'))\\\n",
    "    .select('farm_code', 'site_id', 'sensor', 'reading', 'year', 'month', 'day', 'metrics', 'window')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
