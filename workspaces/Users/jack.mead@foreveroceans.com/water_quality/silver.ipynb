{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b60634d-6454-40fd-bc9d-a588aa25de7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "watermark = spark.conf.get('spark.custom.silver.watermark')\n",
    "bronze_table_name = spark.conf.get('spark.custom.silver.bronze_table_name')\n",
    "table_name = spark.conf.get('spark.custom.silver.table_name')\n",
    "table_path = spark.conf.get('spark.custom.silver.table_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f3f2cc8-bdc5-4e21-a85b-6752fc2bdde7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=table_name,\n",
    "    comment=\"\"\"\n",
    "        C2 Water Quality Sensor Data from Panama Avalon Operations\n",
    "\n",
    "        All data points for water quality sensors: Exo Sonde YSI, Manta Sonde, and the TChain sensor.\n",
    "        This dataset is optimized by cage location (site_id) the year, month, and day. It contains\n",
    "        data from the sensors for the Avalon systems in Panama. This dataset is scheduled to update every day\n",
    "    \"\"\",\n",
    "    path=table_path,\n",
    "    partition_cols=[\"site_id\", \"year\", \"month\", \"day\"],\n",
    "    spark_conf={\n",
    "        'spark.sql.sources.partitionOverwriteMode': 'dynamic',\n",
    "        'spark.sql.session.timeZone': 'UTC'\n",
    "    },\n",
    "    table_properties={\n",
    "        'pipelines.autoOptimize.managed': 'true',\n",
    "        'pipelines.autoOptimize.zOrderCols': 'sensor,timestamp',\n",
    "        'pipelines.reset.allowed': 'false'\n",
    "    }\n",
    ")\n",
    "def silver_table():\n",
    "    df = spark\\\n",
    "        .read\\\n",
    "        .format('delta')\\\n",
    "        .table(bronze_table_name)\n",
    "\n",
    "    # df = df.filter(\"`asset_short_name` IN ('tchain', 'ysi1', 'ysi2', 'ysi3', 'tritonswq01')\")\n",
    "    df = df.filter(\"`assetShortName` IN ('tchain', 'ysi1', 'ysi2', 'ysi3', 'tritonswq01')\")\n",
    "\n",
    "    # df = df.withColumn('date', f.to_date(f.col('timestamp')))\n",
    "\n",
    "    df = df.filter(f'date >= date_sub(current_date(), {watermark})')\n",
    "\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    df = df.withColumn('timestamp', f.to_timestamp(f.from_unixtime(f.col('timestamp'))))\\\n",
    "        .withColumn('year', f.year('timestamp'))\\\n",
    "        .withColumn('month', f.month('timestamp'))\\\n",
    "        .withColumn('day', f.dayofmonth('timestamp'))\n",
    "\n",
    "    # df = df.drop_duplicates()\n",
    "\n",
    "    return df.selectExpr(\n",
    "        # 'asset_short_name as sensor',\n",
    "        # 'reading_short_name as reading',\n",
    "        # 'farm_code',\n",
    "        # 'site_id',\n",
    "        'assetShortName as sensor',\n",
    "        'shortName as reading',\n",
    "        'farmCode as farm_code',\n",
    "        'siteId as site_id',\n",
    "        'value',\n",
    "        'timestamp',\n",
    "        'year',\n",
    "        'month',\n",
    "        'day'\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
