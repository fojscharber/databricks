{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "044bcd14-8932-4fe3-a1df-1e84df78b0bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import json\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "bronze_table_name = spark.conf.get('spark.custom.silver.bronze_table_name')\n",
    "table_name = spark.conf.get('spark.custom.silver.table_name')\n",
    "table_path = spark.conf.get('spark.custom.silver.table_path')\n",
    "watermark = spark.conf.get('spark.custom.silver.watermark')\n",
    "biomass_slope = spark.conf.get('spark.custom.bio.slope')\n",
    "biomass_power = spark.conf.get('spark.custom.bio.power')\n",
    "percent_offset = spark.conf.get('spark.custom.bio.offset')\n",
    "\n",
    "max_depth_difference = 50\n",
    "min_depth = 35\n",
    "max_depth = 140\n",
    "min_length = 5\n",
    "max_length = 50\n",
    "zscore = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0987195-08e7-4680-9d93-5b51cb36e638",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=table_name,\n",
    "    comment=\"\"\"\n",
    "    Filtered Biomass Results\n",
    "\n",
    "    This table has been filtered with different mechanisms to produce only the\n",
    "    true positives: the fish instances that were detected and a weight and\n",
    "    length were produced. This table should be used to compare to manual\n",
    "    samples, and to create aggregations over time.\n",
    "    \"\"\",\n",
    "    path=table_path,\n",
    "    partition_cols=[\"camera_name\", \"year\", \"month\", \"day\"],\n",
    "    spark_conf={\n",
    "        'spark.sql.session.timeZone': 'UTC',\n",
    "        'spark.sql.sources.partitionOverwriteMode': 'dynamic',\n",
    "    },\n",
    "    table_properties={\n",
    "        'pipelines.autoOptimize.managed': 'true',\n",
    "        'pipelines.autoOptimize.zOrderCols': 'timestamp'\n",
    "    }\n",
    ")\n",
    "def silver_data():\n",
    "    df = spark\\\n",
    "        .read\\\n",
    "        .format('delta')\\\n",
    "        .table(bronze_table_name)\n",
    "    \n",
    "    df = df.withColumn('timestamp', f.to_timestamp(f.from_unixtime(f.col('timestamp') / 1000)))\n",
    "    df = df.withColumn('date', f.to_date(f.col('timestamp'), 'yyy-MM-dd'))\n",
    "    \n",
    "    # filter so we don't reprocess the entire dataset\n",
    "    df = df.filter(f'date >= date_sub(current_date(), {watermark})')\n",
    "\n",
    "    # drop duplicate instances\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # partition on the date because it's set using a day interval\n",
    "    camera_model_partition = Window.partitionBy(\n",
    "        f.col(\"cameraName\"),\n",
    "        f.col(\"modelVersion\"),\n",
    "        f.col(\"date\")\n",
    "    )\n",
    "    \n",
    "    # filter invalid instances. Length was not detected\n",
    "    df = df.filter('length3d is not null')\n",
    "    \n",
    "    # filter based on difference between the max and min depth. The difference\n",
    "    # between the max and min depth shouldn't be drastically different due to\n",
    "    # the fact that the fish should be at a given orientation and pose that is\n",
    "    # \"mostly\" parallel to the camera\n",
    "    df = df.filter(f'maxDepth - minDepth <= {max_depth_difference}')\n",
    "    \n",
    "    # filter based on a depth range. The stereo camera works most effectively\n",
    "    # when the fish is positioned at a certain range of depths.\n",
    "    df = df.filter(f'avgDepth > {min_depth} and avgDepth < {max_depth}')\n",
    "    \n",
    "    # filter based on a length range. A simple test of min and max length can\n",
    "    # remove outliers for fish that can never grow beyond a certain range of lengths\n",
    "    # anyways.\n",
    "    df = df.filter(f'length3d > {min_length} and length3d < {max_length}')\n",
    "    \n",
    "    # filter out lengths not within a certain z-score from the normal over a day period. This is\n",
    "    # a simple test to remove outliers based on pure statistics.\n",
    "    df = df.withColumn('medianLength', f.percentile_approx('length3d', 0.5).over(camera_model_partition))\n",
    "    df = df.withColumn('stdDevLength', f.stddev('length3d').over(camera_model_partition))\n",
    "    df = df.withColumn('minLength', f.col('medianLength') - zscore * f.col('stdDevLength'))\n",
    "    df = df.withColumn('maxLength', f.col('medianLength') + zscore * f.col('stdDevLength'))\n",
    "    df = df.filter('medianLength > minLength and medianLength < maxLength')\n",
    "    \n",
    "    df = df.selectExpr(\n",
    "        'cameraName as camera_name',\n",
    "        'fileUrl as file_url',\n",
    "        'uniqueKey as unique_key',\n",
    "        'modelVersion as model_version',\n",
    "        'instanceIndex as instance_index',\n",
    "        'timestamp',\n",
    "        'boundingBox as bounding_box',\n",
    "        'maskSize as total_mask_pixels',\n",
    "        'depthSize as total_depth_pixels',\n",
    "        'pixelLength as pixel_length',\n",
    "        'stdDepth as std_depth',\n",
    "        'nosePoint as nose_point',\n",
    "        'noseDepth as nose_depth',\n",
    "        'tailPoint as tail_point',\n",
    "        'tailDepth as tail_depth',\n",
    "        'avgDepth as avg_depth',\n",
    "        'length3d as length',\n",
    "        'minDepth as min_depth',\n",
    "        'maxDepth as max_depth',\n",
    "    )\n",
    "    \n",
    "    # offset the length using a static percent offset rule\n",
    "    df = df.withColumn('offset_length', f.col('length') * percent_offset)\n",
    "    \n",
    "    # Compute the biomass from the length! Yay\n",
    "    df = df.withColumn('offset_weight', biomass_slope * (f.col('offset_length') ** biomass_power))\n",
    "    df = df.withColumn('weight', biomass_slope * (f.col('length') ** biomass_power))\n",
    "    \n",
    "    df = df.withColumn('year', f.year('timestamp'))\\\n",
    "        .withColumn('month', f.month('timestamp'))\\\n",
    "        .withColumn('day', f.dayofmonth('timestamp'))\n",
    "    \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
