{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93d61b66-7e6a-480a-a633-baf2ddbc3d96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json, os, datetime, requests\n",
    "import requests.packages.urllib3\n",
    "\n",
    "global pprint_j\n",
    "\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "\n",
    "# Helper to pretty print json\n",
    "def pprint_j(i):\n",
    "    print(json.dumps(i, indent=4, sort_keys=True))\n",
    "\n",
    "\n",
    "class dbclient:\n",
    "    \"\"\"\n",
    "    Rest API Wrapper for Databricks APIs\n",
    "    \"\"\"\n",
    "    # set of http error codes to throw an exception if hit. Handles client and auth errors\n",
    "    http_error_codes = (401, 403)\n",
    "\n",
    "    def __init__(self, token, url):\n",
    "        self._token = {'Authorization': 'Bearer {0}'.format(token)}\n",
    "        self._url = url.rstrip(\"/\")\n",
    "        self._is_verbose = False\n",
    "        self._verify_ssl = False\n",
    "        if self._verify_ssl:\n",
    "            # set these env variables if skip SSL verification is enabled\n",
    "            os.environ['REQUESTS_CA_BUNDLE'] = \"\"\n",
    "            os.environ['CURL_CA_BUNDLE'] = \"\"\n",
    "\n",
    "    def is_aws(self):\n",
    "        return self._is_aws\n",
    "\n",
    "    def is_verbose(self):\n",
    "        return self._is_verbose\n",
    "\n",
    "    def is_skip_failed(self):\n",
    "        return self._skip_failed\n",
    "\n",
    "    def test_connection(self):\n",
    "        # verify the proper url settings to configure this client\n",
    "        if self._url[-4:] != '.com' and self._url[-4:] != '.net':\n",
    "            print(\"Hostname should end in '.com'\")\n",
    "            return -1\n",
    "        results = requests.get(self._url + '/api/2.0/clusters/spark-versions', headers=self._token,\n",
    "                               verify=self._verify_ssl)\n",
    "        http_status_code = results.status_code\n",
    "        if http_status_code != 200:\n",
    "            print(\"Error. Either the credentials have expired or the credentials don't have proper permissions.\")\n",
    "            print(\"If you have a ~/.netrc file, check those credentials. Those take precedence over passed input.\")\n",
    "            print(results.text)\n",
    "            return -1\n",
    "        return 0\n",
    "\n",
    "    def get(self, endpoint, json_params=None, version='2.0', print_json=False):\n",
    "        if version:\n",
    "            ver = version\n",
    "        full_endpoint = self._url + '/api/{0}'.format(ver) + endpoint\n",
    "        if self.is_verbose():\n",
    "            print(\"Get: {0}\".format(full_endpoint))\n",
    "        if json_params:\n",
    "            raw_results = requests.get(full_endpoint, headers=self._token, params=json_params, verify=self._verify_ssl)\n",
    "            http_status_code = raw_results.status_code\n",
    "            if http_status_code in dbclient.http_error_codes:\n",
    "                raise Exception(\"Error: GET request failed with code {}\\n{}\".format(http_status_code, raw_results.text))\n",
    "            results = raw_results.json()\n",
    "        else:\n",
    "            raw_results = requests.get(full_endpoint, headers=self._token, verify=self._verify_ssl)\n",
    "            http_status_code = raw_results.status_code\n",
    "            if http_status_code in dbclient.http_error_codes:\n",
    "                raise Exception(\"Error: GET request failed with code {}\\n{}\".format(http_status_code, raw_results.text))\n",
    "            results = raw_results.json()\n",
    "        if print_json:\n",
    "            print(json.dumps(results, indent=4, sort_keys=True))\n",
    "        if type(results) == list:\n",
    "            results = {'elements': results}\n",
    "        results['http_status_code'] = raw_results.status_code\n",
    "        return results\n",
    "\n",
    "    def http_req(self, http_type, endpoint, json_params, version='2.0', print_json=False, files_json=None):\n",
    "        if version:\n",
    "            ver = version\n",
    "        full_endpoint = self._url + '/api/{0}'.format(ver) + endpoint\n",
    "        if self.is_verbose():\n",
    "            print(\"{0}: {1}\".format(http_type, full_endpoint))\n",
    "        if json_params:\n",
    "            if http_type == 'post':\n",
    "                if files_json:\n",
    "                    raw_results = requests.post(full_endpoint, headers=self._token,\n",
    "                                                data=json_params, files=files_json, verify=self._verify_ssl)\n",
    "                else:\n",
    "                    raw_results = requests.post(full_endpoint, headers=self._token,\n",
    "                                                json=json_params, verify=self._verify_ssl)\n",
    "            if http_type == 'put':\n",
    "                raw_results = requests.put(full_endpoint, headers=self._token,\n",
    "                                           json=json_params, verify=self._verify_ssl)\n",
    "            if http_type == 'patch':\n",
    "                raw_results = requests.patch(full_endpoint, headers=self._token,\n",
    "                                             json=json_params, verify=self._verify_ssl)\n",
    "            \n",
    "            http_status_code = raw_results.status_code\n",
    "            if http_status_code in dbclient.http_error_codes:\n",
    "                raise Exception(\"Error: {0} request failed with code {1}\\n{2}\".format(http_type,\n",
    "                                                                                      http_status_code,\n",
    "                                                                                      raw_results.text))\n",
    "            results = raw_results.json()\n",
    "        else:\n",
    "            print(\"Must have a payload in json_args param.\")\n",
    "            return {}\n",
    "        if print_json:\n",
    "            print(json.dumps(results, indent=4, sort_keys=True))\n",
    "        # if results are empty, let's return the return status\n",
    "        if results:\n",
    "            results['http_status_code'] = raw_results.status_code\n",
    "            return results\n",
    "        else:\n",
    "            return {'http_status_code': raw_results.status_code}\n",
    "\n",
    "    def post(self, endpoint, json_params, version='2.0', print_json=False, files_json=None):\n",
    "        return self.http_req('post', endpoint, json_params, version, print_json, files_json)\n",
    "\n",
    "    def put(self, endpoint, json_params, version='2.0', print_json=False):\n",
    "        return self.http_req('put', endpoint, json_params, version, print_json)\n",
    "\n",
    "    def patch(self, endpoint, json_params, version='2.0', print_json=False):\n",
    "        return self.http_req('patch', endpoint, json_params, version, print_json)\n",
    "\n",
    "    @staticmethod\n",
    "    def my_map(F, items):\n",
    "        to_return = []\n",
    "        for elem in items:\n",
    "            to_return.append(F(elem))\n",
    "        return to_return\n",
    "\n",
    "    def set_export_dir(self, dir_location):\n",
    "        self._export_dir = dir_location\n",
    "\n",
    "    def get_export_dir(self):\n",
    "        return self._export_dir\n",
    "\n",
    "    def get_latest_spark_version(self):\n",
    "        versions = self.get('/clusters/spark-versions')['versions']\n",
    "        v_sorted = sorted(versions, key=lambda i: i['key'], reverse=True)\n",
    "        for x in v_sorted:\n",
    "            img_type = x['key'].split('-')[1][0:5]\n",
    "            if img_type == 'scala':\n",
    "                return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc00769-6560-4c52-a4b9-6986fb388ccd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class migrateclient(dbclient):\n",
    "    \n",
    "    def get_num_defined_jobs(self):\n",
    "      jobs_list = self.get('/jobs/list').get('jobs', [])\n",
    "      return len(jobs_list)\n",
    "    \n",
    "    def get_num_external_jobs(self):\n",
    "      job_runs = self.get('/jobs/runs/list').get('runs', [])\n",
    "      job_ids_list = set(map(lambda x: x.get('job_id', None), self.get('/jobs/list').get('jobs', [])))\n",
    "      job_ids_from_runs = set(map(lambda x: x.get('job_id', None), job_runs))\n",
    "      ephemeral_job_ids = job_ids_from_runs - job_ids_list \n",
    "      return len(ephemeral_job_ids)\n",
    "    \n",
    "    def get_num_users(self):\n",
    "      users = self.get('/preview/scim/v2/Users').get('Resources', [])\n",
    "      return len(users)\n",
    "    \n",
    "    def get_num_groups(self):\n",
    "      groups = self.get('/preview/scim/v2/Groups').get('Resources', [])\n",
    "      return len(groups)\n",
    "    \n",
    "    def get_num_notebooks(self, second_level=False):\n",
    "      users = self.get('/preview/scim/v2/Users').get('Resources', [])\n",
    "      total_nbs = 0 \n",
    "      second_level_dirs = []\n",
    "      for user in users:\n",
    "        path = '/Users/' + user['userName']\n",
    "        ls = self.get('/workspace/list', {'path' : path}).get('objects', [])\n",
    "        nbs = list(filter(lambda x: x.get('object_type', None) == 'NOTEBOOK', ls))\n",
    "        total_nbs += len(nbs) \n",
    "        dirs = list(filter(lambda x: x.get('object_type', None) == 'DIRECTORY', ls))\n",
    "        for p in dirs:\n",
    "          dir_path = p.get('path')\n",
    "          ls_dir = self.get('/workspace/list', {'path' : dir_path}).get('objects', [])\n",
    "          dir_nbs = list(filter(lambda x: x.get('object_type', None) == 'NOTEBOOK', ls_dir))\n",
    "          second_level_dirs.extend(filter(lambda x: x.get('object_type', None) == 'DIRECTORY', ls_dir))\n",
    "          total_nbs += len(dir_nbs) \n",
    "      # search 2 levels deep only to get an approximate notebook count\n",
    "      if second_level:\n",
    "        for p in second_level_dirs:\n",
    "          dir_path = p.get('path')\n",
    "          ls_dir = self.get('/workspace/list', {'path' : dir_path}).get('objects', [])\n",
    "          dir_nbs = list(filter(lambda x: x.get('object_type', None) == 'NOTEBOOK', ls_dir))\n",
    "          total_nbs += len(dir_nbs) \n",
    "      return total_nbs \n",
    "        \n",
    "    def get_num_databases(self):\n",
    "      dbs = spark.catalog.listDatabases()\n",
    "      return len(dbs)\n",
    "    \n",
    "    def get_num_tables(self):\n",
    "      dbs = spark.catalog.listDatabases()\n",
    "      table_count = 0\n",
    "      for db in dbs:\n",
    "        tables = spark.catalog.listTables(db.name)\n",
    "        table_count += len(tables)\n",
    "      return table_count \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09e559f-bc47-45cf-ad48-6568d8aeecd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Manage & External Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3b33c2f-493a-4830-a36e-b98051e17883",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Managed_Table_Count = 0\n",
    "# External_Table_Count = 0\n",
    "# Error_Table_Count = 0\n",
    "\n",
    "# db_list = [d[0] for d in spark.catalog.listDatabases()]\n",
    "# for db in db_list:\n",
    "#     tableList = [t[0] for t in spark.catalog.listTables(dbName = db)]\n",
    "#     for table in tableList:\n",
    "#       try:\n",
    "#         table_details = sql(\"describe extended {}.{}\".format(db, table))\n",
    "#         table_type = table_details.filter(table_details.col_name == \"Type\").collect()[0][1]\n",
    "#         if table_type.upper()=='MANAGED':\n",
    "#           Managed_Table_Count+=1\n",
    "#         elif table_type.upper()=='EXTERNAL':\n",
    "#           External_Table_Count+=1\n",
    "#       except:\n",
    "#         Error_Table_Count+=1\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a27c647-8c32-48cd-8502-8fc4a1aaac92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None) \n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "\n",
    "client = migrateclient(token, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d36bb691-4021-4344-a2fa-6267edc6723f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of users:  17\nNum of groups:  4\nApproximate num of notebooks:  47\nNum of internal jobs:  7\nNum of external jobs:  0\nNum of databases:  8\nNum of tables:  601\n"
     ]
    }
   ],
   "source": [
    "print(\"Num of users: \", client.get_num_users())\n",
    "print(\"Num of groups: \", client.get_num_groups())\n",
    "print(\"Approximate num of notebooks: \", client.get_num_notebooks(True))\n",
    "print(\"Num of internal jobs: \", client.get_num_defined_jobs())\n",
    "print(\"Num of external jobs: \", client.get_num_external_jobs())\n",
    "print(\"Num of databases: \", client.get_num_databases())\n",
    "print(\"Num of tables: \", client.get_num_tables())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05e4259d-2040-47df-a80e-66a46a2a94c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Workspace Analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

