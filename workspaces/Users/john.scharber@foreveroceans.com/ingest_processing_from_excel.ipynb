{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37698786-f359-470f-9b36-7276bdfdabb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.10/site-packages (1.5.3)\nRequirement already satisfied: xlrd in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fb895e91-5a91-4951-b108-af63126bccc3/lib/python3.10/site-packages (2.0.1)\nRequirement already satisfied: openpyxl in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fb895e91-5a91-4951-b108-af63126bccc3/lib/python3.10/site-packages (3.1.2)\nRequirement already satisfied: numpy>=1.21.0 in /databricks/python3/lib/python3.10/site-packages (from pandas) (1.23.5)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas) (2022.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: et-xmlfile in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fb895e91-5a91-4951-b108-af63126bccc3/lib/python3.10/site-packages (from openpyxl) (1.1.0)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install pandas xlrd openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a5bf31-afa0-425a-8e48-840cb3649ac7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 129 entries, 0 to 128\nData columns (total 14 columns):\n #   Column               Non-Null Count  Dtype         \n---  ------               --------------  -----         \n 0   date                 129 non-null    datetime64[ns]\n 1   count2To4            129 non-null    int64         \n 2   count4To6            128 non-null    float64       \n 3   count6Plus           123 non-null    float64       \n 4   finalWeightLbs2To4   129 non-null    float64       \n 5   finalWeightLbs4To6   128 non-null    float64       \n 6   finalWeightLbs6Plus  123 non-null    float64       \n 7   weight2To4           129 non-null    float64       \n 8   weight4To6           128 non-null    float64       \n 9   weight6Plus          123 non-null    float64       \n 10  avgWeight2To4        107 non-null    float64       \n 11  avgWeight4To6        128 non-null    float64       \n 12  avgWeight6Plus       116 non-null    float64       \n 13  cohort               129 non-null    object        \ndtypes: datetime64[ns](1), float64(11), int64(1), object(1)\nmemory usage: 14.2+ KB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas\n",
    "\n",
    "file_path = \"/Workspace/Users/john.scharber@foreveroceans.com/biomass trueup/Harvest Data.xlsx\"\n",
    "column_names = [\n",
    "    'date',\n",
    "\n",
    "    'count2To4',\n",
    "    'count4To6',\n",
    "    'count6Plus',\n",
    "\n",
    "    'finalWeightLbs2To4',\n",
    "    'finalWeightLbs4To6',\n",
    "    'finalWeightLbs6Plus',\n",
    "\n",
    "    'weight2To4',\n",
    "    'weight4To6',\n",
    "    'weight6Plus',\n",
    "\n",
    "    'avgWeight2To4',\n",
    "    'avgWeight4To6',\n",
    "    'avgWeight6Plus',\n",
    "\n",
    "    'cohort']\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "pdf = pandas.read_excel(file_path)\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "df.write.mode(\"overwrite\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .option(\"mergeSchema\", \"true\")\\\n",
    "    .partitionBy(\"cohort\", \"date\")\\\n",
    "    .saveAsTable(\"hive_metastore.default.harvetProcessingData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9572e4e0-56e8-4607-867b-a865ac8dcbc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestProcessingDataFromExcel",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

